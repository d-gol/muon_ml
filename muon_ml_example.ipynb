{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from keras.layers import Input, Activation, BatchNormalization, Dense, Conv2D, concatenate, Reshape, MaxPooling2D\n",
    "from keras import Model, Sequential\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.losses import logcosh, mse, mae, mape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import load_model, model_from_json\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "import h5py\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "\n",
    "\n",
    "def save_model(model_save_name, model):\n",
    "    with open(model_save_name + '.json', 'w') as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "\n",
    "    model.save_weights(model_save_name + '.h5')\n",
    "    \n",
    "    \n",
    "def load_model_files(model_name):\n",
    "    json_file = open(model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(model_name + '.h5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class MuonML:\n",
    "    def __init__(self, params, experiment_name=None, X_train=None, Y_train=None, X_test=None, Y_test=None):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        \n",
    "        self.name = experiment_name\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.X_scaler = None\n",
    "        self.Y_scaler = None\n",
    "        \n",
    "        self.params = params\n",
    "        \n",
    "        self.models = []\n",
    "        self.histories = []\n",
    "        self.results = []\n",
    "        self.models_info = None\n",
    "        \n",
    "        print('Creating experiments...')\n",
    "        self.experiments = [dict(zip(self.params.keys(), value)) for value in product(*self.params.values())]\n",
    "        print(len(self.experiments))\n",
    "        \n",
    "        self.__create_list_of_models()\n",
    "        self.__create_model_info()\n",
    "        if X_train != None:\n",
    "            self.__create_folder_structure(self.name)\n",
    "        \n",
    "        print('Created ' + str(len(self.experiments)) + ' models')\n",
    "    \n",
    "    \n",
    "    def __create_folder_structure(self, name):\n",
    "        \"\"\" Creates a folder structure to save info about models \"\"\"\n",
    "        \n",
    "        if not os.path.exists(name):\n",
    "            os.makedirs(name)\n",
    "            \n",
    "        for i, model in enumerate(self.models):\n",
    "            folder_name = name + '/' + 'model_' + str(i)\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "        \n",
    "        \n",
    "    def __create_model_info(self):\n",
    "        \"\"\" Creates a dataframe which provides description of all different models \"\"\"\n",
    "        \n",
    "        data = []\n",
    "        for i, exp in enumerate(self.experiments):\n",
    "            str_values = []\n",
    "            for value in exp.values():\n",
    "                str_values.append(str(value))\n",
    "            data.append(str_values)\n",
    "        data = np.array(data)\n",
    "        \n",
    "        self.models_info = pd.DataFrame(data=data, columns=self.params.keys())\n",
    "        self.models_info.index.name = 'model_id'\n",
    "    \n",
    "    \n",
    "    def __create_list_of_models(self):\n",
    "        \"\"\" Creates a grid from params and makes a list of models \"\"\"\n",
    "        \n",
    "        self.models = [] # list of keras models\n",
    "        \n",
    "        for exp in self.experiments:\n",
    "            print(self.experiments.index(exp))\n",
    "            k_reg = exp['kernel_regularizer'](exp['regularizer_factor'])\n",
    "            a_reg = exp['activity_regularizer'](exp['regularizer_factor'])\n",
    "\n",
    "            inputA = Input(shape = (38, 20, 1))\n",
    "            inputB = Input(shape = (38, 20, 1))\n",
    "\n",
    "            #x0 = BatchNormalization(momentum=0.9)(inputA)\n",
    "            x0 = Conv2D(exp['conv_first_nodes'], (3,3))(inputA)\n",
    "            x0 = Activation('relu')(x0)\n",
    "            if exp['bn_0']:\n",
    "                x0 = BatchNormalization()(x0)\n",
    "\n",
    "            x0 = Conv2D(exp['conv_second_nodes'], (3,3))(x0)\n",
    "            x0 = Activation('relu')(x0)\n",
    "            if exp['bn_1']:\n",
    "                x0 = BatchNormalization()(x0)\n",
    "            \n",
    "            x0 = Conv2D(exp['conv_third_nodes'], (3,3))(x0)\n",
    "            x0 = Activation('relu')(x0)\n",
    "            if exp['bn_2']:\n",
    "                x0 = BatchNormalization()(x0)\n",
    "\n",
    "            x0 = MaxPooling2D((2,2), strides=(2,2))(x0)\n",
    "\n",
    "            #x1 = BatchNormalization(momentum=0.9)(inputB)\n",
    "            x1 = Conv2D(exp['conv_first_nodes'], (3,3))(inputB)\n",
    "            x1 = Activation('relu')(x1)\n",
    "            if exp['bn_0']:\n",
    "                x1 = BatchNormalization()(x1)\n",
    "\n",
    "            x1 = Conv2D(exp['conv_second_nodes'], (3,3))(x1)\n",
    "            x1 = Activation('relu')(x1)\n",
    "            if exp['bn_1']:\n",
    "                x1 = BatchNormalization()(x1)\n",
    "\n",
    "            x1 = Conv2D(exp['conv_third_nodes'], (3,3))(x1)\n",
    "            x1 = Activation('relu')(x1)\n",
    "            if exp['bn_2']:\n",
    "                x1 = BatchNormalization()(x1)\n",
    "\n",
    "            x1 = MaxPooling2D((2,2), strides=(2,2))(x1)\n",
    "\n",
    "            x = concatenate([x0, x1])\n",
    "\n",
    "            target_shape = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "            x = Reshape(target_shape=(int(target_shape),))(x)\n",
    "\n",
    "            x = Dense(256, kernel_regularizer=k_reg, activity_regularizer=a_reg)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            if exp['bn_3']:\n",
    "                x = BatchNormalization()(x)\n",
    "\n",
    "            x = Dense(256, kernel_regularizer=k_reg, activity_regularizer=a_reg)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            if exp['bn_4']:\n",
    "                x = BatchNormalization()(x)\n",
    "\n",
    "            x = Dense(1, activation='linear')(x)\n",
    "\n",
    "            model = Model([inputA, inputB], x)\n",
    "            model.compile(loss=exp['loss'], optimizer=exp['optimizer']())\n",
    "            \n",
    "            self.models.append(model)\n",
    "            \n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\" Train all models and save json, h5 and history \"\"\"\n",
    "        \n",
    "        self.histories = []\n",
    "        for i, exp in enumerate(self.experiments):\n",
    "            print('Training model ' + str(i + 1) + '/' + str(len(self.experiments)))\n",
    "            model = self.models[i]\n",
    "            history = model.fit(self.X_train, self.Y_train, epochs=exp['epochs'],\\\n",
    "                        batch_size=exp['batch_size'], verbose=1, validation_split=exp['validation_split'])\n",
    "            self.histories.append(history)\n",
    "            \n",
    "            # Save model info\n",
    "            model_path = self.name + '/' + 'model_' + str(i) + '/model_' + str(i)\n",
    "            save_model(model_path, model)\n",
    "            \n",
    "            # Save model history\n",
    "            with open(model_path + '_history.json', 'w') as f:\n",
    "                json.dump(str(history.history), f)\n",
    "            \n",
    "    \n",
    "    def evaluate_models(self, metrics):\n",
    "        \"\"\" Evaluate all models and save results \"\"\"\n",
    "        \n",
    "        self.results = []\n",
    "        for i, exp in enumerate(self.experiments):\n",
    "            print('Evaluating model ' + str(i + 1) + '/' + \\\n",
    "                  str(len(self.experiments)) + ' for metrics ' + str(metrics))\n",
    "            model = self.models[i]\n",
    "            model.compile(loss=exp['loss'], optimizer=exp['optimizer'](), metrics=metrics)\n",
    "            result = model.evaluate(self.X_test, self.Y_test, batch_size=2048)\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            # Save result to file\n",
    "            model_path = self.name + '/' + 'model_' + str(i) + '/model_' + str(i)\n",
    "            with open(model_path + '_evaluation.csv', 'w') as f:\n",
    "                f.write(str(result)[1:-1])\n",
    "            \n",
    "        results_df = pd.DataFrame(data=self.results, columns=self.models[0].metrics_names)\n",
    "        self.models_info = self.models_info.join(results_df, on='model_id', how='outer', lsuffix='_functions')\n",
    "        self.models_info.to_csv(self.name + '/results_overview.csv')\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        \"\"\" Make predictions from models and save results \"\"\"\n",
    "        \n",
    "        for i, exp in enumerate(self.experiments):\n",
    "            print('Predicting outputs of the current test set, model ' + \\\n",
    "                  str(i + 1) + '/' + str(len(self.experiments)))\n",
    "            \n",
    "            model = self.models[i]\n",
    "            model.compile(loss=exp['loss'], optimizer=exp['optimizer']())\n",
    "            \n",
    "            result = model.predict(self.X_test, batch_size=256)\n",
    "            result = np.array(np.squeeze(result)).T\n",
    "            \n",
    "            model_path = self.name + '/' + 'model_' + str(i) + '/model_' + str(i)\n",
    "            \n",
    "            if self.Y_scaler == None:\n",
    "                np.savetxt(model_path + '_predictions.csv', result, delimiter=',')\n",
    "            else:\n",
    "                np.savetxt(model_path + '_predictions_scaled.csv', result, delimiter=',')\n",
    "                results_rescaled = self.Y_scaler.inverse_transform(result)\n",
    "                np.savetxt(model_path + '_predictions.csv', results_rescaled, delimiter=',')\n",
    "                \n",
    "        \n",
    "    def perform_CV(self, X, Y, metrics, n_fold=5, X_scaler=None, Y_scaler=None, \\\n",
    "                   save_predictions=False, stop_after=None):\n",
    "        \"\"\" Run nfold cross validation \"\"\"\n",
    "        \n",
    "        base_name = self.name\n",
    "        test_set_length = int(len(X[0]) / n_fold)\n",
    "        \n",
    "        cv_info_dfs = []\n",
    "        \n",
    "        for i in range(n_fold):\n",
    "            if stop_after != None and i >= stop_after:\n",
    "                break\n",
    "            \n",
    "            print('Performing ' + str(i + 1) + '/' + str(n_fold) + ' iteration of cross validation.')\n",
    "            \n",
    "            self.name = base_name + '_CV/CV_' + str(i)\n",
    "            \n",
    "            test_start = i * test_set_length\n",
    "            test_end = i * test_set_length + test_set_length\n",
    "            \n",
    "            # Create train/test sets for current fold of CV\n",
    "            X = np.array(X)\n",
    "            \n",
    "            self.X_train = np.concatenate((X[:, 0:test_start, :, :], X[:, test_end:, :, :]), axis=1)\n",
    "            self.Y_train = np.concatenate((Y[0:test_start], Y[test_end:]), axis=0)\n",
    "            self.X_test = X[:, test_start:test_end, :, :]\n",
    "            self.Y_test = Y[test_start:test_end]\n",
    "            \n",
    "            print(self.X_train.shape)\n",
    "            print(self.Y_train.shape)\n",
    "            \n",
    "            print(self.X_test.shape)\n",
    "            print(self.Y_test.shape)\n",
    "            \n",
    "            \n",
    "            self.X_train = list(self.X_train)\n",
    "            self.X_test = list(self.X_test)\n",
    "            \n",
    "            print(len(self.X_train))\n",
    "            print(len(self.X_test))\n",
    "            \n",
    "            # Scale data\n",
    "            if X_scaler != None and Y_scaler != None:\n",
    "                self.X_scaler = X_scaler\n",
    "                self.Y_scaler = Y_scaler\n",
    "            \n",
    "                self.X_train = X_scaler.fit_transform(self.X_train)\n",
    "                self.Y_train = Y_scaler.fit_transform(self.Y_train)\n",
    "\n",
    "                self.X_test = X_scaler.transform(self.X_test)\n",
    "                self.Y_test = Y_scaler.transform(self.Y_test)\n",
    "            \n",
    "            # Setup models for current fold\n",
    "            self.__create_list_of_models()\n",
    "            self.__create_model_info()\n",
    "            self.__create_folder_structure(self.name)\n",
    "            self.train_models()\n",
    "            self.evaluate_models(metrics)\n",
    "            \n",
    "            if save_predictions:\n",
    "                self.predict()\n",
    "            \n",
    "            # Save scalers\n",
    "            joblib.dump(self.X_scaler, self.name + '/X_scaler.dat')\n",
    "            joblib.dump(self.Y_scaler, self.name + '/Y_scaler.dat')\n",
    "            \n",
    "            cv_info_dfs.append(self.models_info)\n",
    "        \n",
    "        # Save average results by model in a dataframe\n",
    "        columns_to_group_by = ['model_id']\n",
    "        for param in self.params.keys():\n",
    "            columns_to_group_by.append(param)\n",
    "        \n",
    "        average_df = pd.concat(cv_info_dfs).groupby(by=columns_to_group_by).mean()\n",
    "        self.models_info = average_df\n",
    "        self.name = base_name + '_CV'\n",
    "        average_df.to_csv(self.name + '/results_overview.csv')\n",
    "    \n",
    "    \n",
    "    def get_model(self, model_id):\n",
    "        return self.models[model_id]\n",
    "    \n",
    "    \n",
    "    def get_model_history(self, model_id):\n",
    "        return self.histories[model_id]\n",
    "    \n",
    "    \n",
    "    def get_model_evaluation(self, model_id):\n",
    "        return self.results[model_id]\n",
    "    \n",
    "    \n",
    "    def get_best_model_id(self, metrics, idmax=False):\n",
    "        if idmax:\n",
    "            return self.models_info[metrics].idxmax()\n",
    "        else:\n",
    "            return self.models_info[metrics].idxmin()\n",
    "        \n",
    "\n",
    "    def load_models_info(self, name):\n",
    "        \"\"\" Load model's info from a folder. If loading from CV, load from a specific fold, not everything \"\"\"\n",
    "        \"\"\" Load: all models h5 files, all models evaluation, all models history, total evaluation \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.models_info = pd.read_csv(self.name + '/results_overview.csv')\n",
    "        \n",
    "        model_dirs = [x[0] for x in os.walk(self.name)]\n",
    "        model_dirs = model_dirs[1:]\n",
    "        print(model_dirs)\n",
    "        \n",
    "        for i, model_dir in enumerate(model_dirs):\n",
    "            model_path = model_dir + '/model_' + str(i)\n",
    "            \n",
    "            if '.ipynb_checkpoints' in model_path:\n",
    "                continue\n",
    "            \n",
    "            model = load_model_files(model_path)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            with open(model_path + '_history.json', 'r') as json_file:\n",
    "                history = json.load(json_file)\n",
    "            self.histories.append(history)\n",
    "            \n",
    "            with open(model_path + '_evaluation.csv', 'r') as f:\n",
    "                result = f.readlines()\n",
    "            self.results.append(list(result))\n",
    "            \n",
    "            \n",
    "    def plot_history_info(self, key):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(lst, name='', rnd=5):\n",
    "    print(name)\n",
    "    print('mean = ' + str(round(np.mean(lst), rnd)))\n",
    "    print('std  = ' + str(round(np.std(lst), rnd)))\n",
    "    print('min  = ' + str(round(np.min(lst), rnd)))\n",
    "    print('max  = ' + str(round(np.max(lst), rnd)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched(X, Y, Y_cl3d):\n",
    "    map_ids_Y = {x[0]: i for i, x in enumerate(Y) if np.any(X[i,:36,:20,:] > 0)} # Set of ids in Y, if E > 0 in X\n",
    "    map_ids_Y_cl3d = {x[0]: i for i, x in enumerate(Y_cl3d)} # Set of ids in Y_cl3d\n",
    "\n",
    "    # Match the data by ids\n",
    "\n",
    "    X_matched = []\n",
    "    Y_matched = []\n",
    "    Y_cl3d_matched = []\n",
    "\n",
    "    for id_Y in map_ids_Y:\n",
    "        if id_Y in map_ids_Y_cl3d:\n",
    "            ind_Y = map_ids_Y[id_Y]\n",
    "            ind_Y_cl3d = map_ids_Y_cl3d[id_Y]\n",
    "\n",
    "            X_matched.append(X[ind_Y, : , :20, :])\n",
    "            Y_matched.append(Y[ind_Y, 4])\n",
    "\n",
    "            Y_cl3d_matched.append(Y_cl3d[ind_Y_cl3d, 2])\n",
    "\n",
    "    return X_matched, Y_matched, Y_cl3d_matched\n",
    "\n",
    "files = [x for x in os.listdir('/home/dejan_notebooks/datasets/') if 'tc_dat_100' in x]\n",
    "\n",
    "X_matched = []\n",
    "Y_matched = []\n",
    "Y_cl3d_matched = []\n",
    "\n",
    "for filename in files:\n",
    "    filename = '/home/dejan_notebooks/datasets/' + filename\n",
    "    pileup_f = h5py.File(filename, 'r')\n",
    "    X = pileup_f['Cl2d'][:]\n",
    "    Y = pileup_f['Particle'][:] \n",
    "    Y_cl3d = pileup_f['Cl3d'][:]\n",
    "    X = np.array(X)\n",
    "    X = X.squeeze(1)\n",
    "    Y = np.array(Y)\n",
    "    Y_cl3d = np.array(Y_cl3d)\n",
    "    \n",
    "    X_m, Y_m, Y_cl3d_m = get_matched(X, Y, Y_cl3d)\n",
    "    X_matched.extend(X_m)\n",
    "    Y_matched.extend(Y_m)\n",
    "    Y_cl3d_matched.extend(Y_cl3d_m)\n",
    "    \n",
    "    pileup_f.close()\n",
    "\n",
    "X_matched = np.array(X_matched)\n",
    "Y_matched = np.array(Y_matched)\n",
    "Y_cl3d_matched = np.array(Y_cl3d_matched)\n",
    "\n",
    "print(X_matched.shape)\n",
    "print(Y_matched.shape)\n",
    "print(Y_cl3d_matched.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_view_0 = X_matched.sum(axis=2)\n",
    "X_view_1 = X_matched.sum(axis=3)\n",
    "\n",
    "X_view_0_scaled = X_view_0 / 270\n",
    "X_view_1_scaled = X_view_1 / 270\n",
    "\n",
    "X_view_0 = np.expand_dims(X_view_0, -1)\n",
    "X_view_1 = np.expand_dims(X_view_1, -1)\n",
    "\n",
    "X_view_0_scaled = np.expand_dims(X_view_0_scaled, -1)\n",
    "X_view_1_scaled = np.expand_dims(X_view_1_scaled, -1)\n",
    "\n",
    "Y_matched_scaled = Y_matched / 1500\n",
    "Y_cl3d_matched_scaled = Y_cl3d_matched / 1500\n",
    "\n",
    "train_length = int(0.7 * len(Y_matched))\n",
    "\n",
    "Y_train = Y_matched[:train_length]\n",
    "Y_test = Y_matched[train_length:]\n",
    "\n",
    "Y_cl3d_train = Y_cl3d_matched[:train_length]\n",
    "Y_cl3d_test = Y_cl3d_matched[train_length:]\n",
    "\n",
    "X_view_0_train = X_view_0[:train_length]\n",
    "X_view_0_test = X_view_0[train_length:]\n",
    "\n",
    "X_view_1_train = X_view_1[:train_length]\n",
    "X_view_1_test = X_view_1[train_length:]\n",
    "\n",
    "# Scaling\n",
    "\n",
    "Y_train_scaled = Y_matched_scaled[:train_length]\n",
    "Y_test_scaled = Y_matched_scaled[train_length:]\n",
    "\n",
    "Y_cl3d_train_scaled = Y_cl3d_matched_scaled[:train_length]\n",
    "Y_cl3d_test_scaled = Y_cl3d_matched_scaled[train_length:]\n",
    "\n",
    "X_view_0_train_scaled = X_view_0_scaled[:train_length]\n",
    "X_view_0_test_scaled = X_view_0_scaled[train_length:]\n",
    "\n",
    "X_view_1_train_scaled = X_view_1_scaled[:train_length]\n",
    "X_view_1_test_scaled = X_view_1_scaled[train_length:]\n",
    "\n",
    "X_test_summed_E = np.apply_over_axes(np.sum, X_matched[train_length:], [1,2,3]).squeeze((1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=20, cooldown=1, verbose=1, min_lr=0.0000001)\n",
    "loss_lr = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=20, cooldown=1, verbose=1, min_lr=0.0000001)\n",
    "\n",
    "params = {\n",
    "    'activation': [relu],\n",
    "    'batch_size': [1024],\n",
    "    'loss': [logcosh],\n",
    "    'epochs': [300],\n",
    "    'optimizer': [keras.optimizers.Adam],\n",
    "    'validation_split': [0.2],\n",
    "    'kernel_regularizer': [keras.regularizers.l2],\n",
    "    'activity_regularizer': [keras.regularizers.l2],\n",
    "    'regularizer_factor': [0.001, 0.0000001],\n",
    "    'conv_first_nodes': [8, 16],\n",
    "    'conv_second_nodes': [32, 64],\n",
    "    'conv_third_nodes': [128, 256],\n",
    "    'bn_0': [True, ],\n",
    "    'bn_1': [True, ],\n",
    "    'bn_2': [True, ],\n",
    "    'bn_3': [True, ],\n",
    "    'bn_4': [True, ],\n",
    "    'callbacks': [[loss_lr, val_loss_lr]]\n",
    "}\n",
    "\n",
    "metrics=['mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MuonML(params, 'hgcal_first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.perform_CV([X_view_0, X_view_1], Y_matched, metrics, n_fold=5, X_scaler=None, Y_scaler=None, \\\n",
    "             save_predictions=True, stop_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, history in enumerate(m.histories):\n",
    "    print(i)\n",
    "    history = history.history\n",
    "    m.models_info.iloc[i]\n",
    "    #print(m.models_info.iloc[i]['losses'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    #plt.ylim((0.0014, 0.0025))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
